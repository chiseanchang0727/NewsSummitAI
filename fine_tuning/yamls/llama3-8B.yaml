Llama3-8B:
  model_name: hfl/llama-3-chinese-8b
  bnb_config:
    load_in_4bit: True
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: torch.bfloat16
    bnb_4bit_use_double_quant: True
  lora_config:
    r: 16
    lora_alpha: 16
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
    bias: "none"
    lora_dropout: 0.05
    task_type: "CAUSAL_LM"
  peft_training_args:
    output_dir: "results"
    warmup_steps: 1
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 4
    max_steps: 10
    learning_rate: 2e-4
    optim: "paged_adamw_8bit"
    logging_steps: 25
    logging_dir: "./logs"
    save_strategy: "steps"
    save_steps: 25
    evaluation_strategy: "steps"
    do_eval: true
    gradient_checkpointing: true
    report_to: "none"
    overwrite_output_dir: true
    group_by_length: true
